{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "from long_caption import LongCaptioner\n",
    "from short_tag import ShortTagger\n",
    "from milabol import Milabol\n",
    "\n",
    "# model_name_or_path = '/data02/models/blip-large-long-cap'\n",
    "model_name_or_path = '/data02/models/blip2-opt-2.7b'\n",
    "wd_model_name_or_path = '/data02/models/wd-swinv2-tagger-v3'\n",
    "device = 'cuda:6'\n",
    "\n",
    "data_path = '/data02/users/lz/code/lora-scripts/data/mfs'\n",
    "repeat_name = '30_mfs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "# # 遍历根文件夹下的所有文件和文件夹\n",
    "# for root, dirs, files in os.walk(data_path):\n",
    "#     for file in files:\n",
    "#         # 判断文件是否为图片文件（可以根据需要扩展图片格式）\n",
    "#         if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "#             # 构建源文件路径和目标文件路径\n",
    "#             source_file = os.path.join(root, file)\n",
    "#             target_file = os.path.join(data_path, repeat_name, f'{counter}.{os.path.splitext(source_file)[1]}')\n",
    "#             os.makedirs(os.path.dirname(target_file),exist_ok=True)\n",
    "#             # 移动文件\n",
    "#             shutil.move(source_file, target_file)\n",
    "#             counter += 1\n",
    "\n",
    "# for root, dirs, files in os.walk(data_path, topdown=False):\n",
    "#     for name in dirs:\n",
    "#         if '_' not in name:\n",
    "#             # 构建文件夹的完整路径\n",
    "#             folder_path = os.path.join(root, name)\n",
    "#             # 删除文件夹\n",
    "#             os.rmdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '__getstate__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m milaboler \u001b[39m=\u001b[39m Milabol(model_name_or_path, wd_model_name_or_path, device)\n",
      "File \u001b[0;32m/data02/users/lz/code/Milabol/milabol.py:9\u001b[0m, in \u001b[0;36mMilabol.__init__\u001b[0;34m(self, blip_path, wd_path, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, blip_path, wd_path, device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlc \u001b[39m=\u001b[39m LongCaptioner(blip_path, device)\n\u001b[1;32m     10\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mst \u001b[39m=\u001b[39m ShortTagger(wd_path)\n",
      "File \u001b[0;32m/data02/users/lz/code/Milabol/long_caption.py:7\u001b[0m, in \u001b[0;36mLongCaptioner.__init__\u001b[0;34m(self, model_name_or_path, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model_name_or_path, device\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor \u001b[39m=\u001b[39m BlipProcessor\u001b[39m.\u001b[39;49mfrom_pretrained(model_name_or_path)\n\u001b[1;32m      8\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m BlipForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(model_name_or_path)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/data02/users/lz/miniconda3/envs/UICoder/lib/python3.10/site-packages/transformers/processing_utils.py:458\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m token\n\u001b[0;32m--> 458\u001b[0m args \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_arguments_from_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    460\u001b[0m \u001b[39m# Existing processors on the Hub created before #27761 being merged don't have `processor_config.json` (if not\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[39m# updated afterward), and we need to keep `from_pretrained` work. So here it fallbacks to the empty dict.\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[39m# However, for models added in the future, we won't get the expected error if this file is missing.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data02/users/lz/miniconda3/envs/UICoder/lib/python3.10/site-packages/transformers/processing_utils.py:514\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m         attribute_class \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[0;32m--> 514\u001b[0m     args\u001b[39m.\u001b[39mappend(attribute_class\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    515\u001b[0m \u001b[39mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m/data02/users/lz/miniconda3/envs/UICoder/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2029\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2026\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2027\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2029\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   2030\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   2031\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   2032\u001b[0m     init_configuration,\n\u001b[1;32m   2033\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   2034\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2035\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2036\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2037\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   2038\u001b[0m     _is_local\u001b[39m=\u001b[39;49mis_local,\n\u001b[1;32m   2039\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2040\u001b[0m )\n",
      "File \u001b[0;32m/data02/users/lz/miniconda3/envs/UICoder/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2261\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2259\u001b[0m \u001b[39m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2261\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   2262\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   2263\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   2264\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2265\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2266\u001b[0m     )\n",
      "File \u001b[0;32m/data02/users/lz/miniconda3/envs/UICoder/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert_fast.py:235\u001b[0m, in \u001b[0;36mBertTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, do_lower_case, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    208\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    209\u001b[0m     vocab_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    220\u001b[0m ):\n\u001b[1;32m    221\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m    222\u001b[0m         vocab_file,\n\u001b[1;32m    223\u001b[0m         tokenizer_file\u001b[39m=\u001b[39mtokenizer_file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    233\u001b[0m     )\n\u001b[0;32m--> 235\u001b[0m     normalizer_state \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackend_tokenizer\u001b[39m.\u001b[39;49mnormalizer\u001b[39m.\u001b[39;49m__getstate__())\n\u001b[1;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    237\u001b[0m         normalizer_state\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m\"\u001b[39m, do_lower_case) \u001b[39m!=\u001b[39m do_lower_case\n\u001b[1;32m    238\u001b[0m         \u001b[39mor\u001b[39;00m normalizer_state\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstrip_accents\u001b[39m\u001b[39m\"\u001b[39m, strip_accents) \u001b[39m!=\u001b[39m strip_accents\n\u001b[1;32m    239\u001b[0m         \u001b[39mor\u001b[39;00m normalizer_state\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mhandle_chinese_chars\u001b[39m\u001b[39m\"\u001b[39m, tokenize_chinese_chars) \u001b[39m!=\u001b[39m tokenize_chinese_chars\n\u001b[1;32m    240\u001b[0m     ):\n\u001b[1;32m    241\u001b[0m         normalizer_class \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(normalizers, normalizer_state\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '__getstate__'"
     ]
    }
   ],
   "source": [
    "milaboler = Milabol(model_name_or_path, wd_model_name_or_path, device, version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2096 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1004/2096 [33:46<47:44,  2.62s/it] /data02/users/lz/miniconda3/envs/UICoder/lib/python3.10/site-packages/PIL/Image.py:3186: DecompressionBombWarning: Image size (98621216 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 1271/2096 [42:24<29:05,  2.12s/it]  /data02/users/lz/miniconda3/envs/UICoder/lib/python3.10/site-packages/PIL/Image.py:3186: DecompressionBombWarning: Image size (103578664 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      " 94%|█████████▍| 1972/2096 [1:05:07<04:25,  2.14s/it]/data02/users/lz/miniconda3/envs/UICoder/lib/python3.10/site-packages/PIL/Image.py:3186: DecompressionBombWarning: Image size (101090736 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "100%|██████████| 2096/2096 [1:09:08<00:00,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "milaboler.run(os.path.join(data_path,repeat_name),triggers=\"#milabo, #memphis\", general_thresh=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
